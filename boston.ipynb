{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "7pLnq_GNdg23"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1.0 - sigmoid(z))\n",
    "\n",
    "def train(X, y, n_hidden, learning_rate, n_iter):\n",
    "    m, n_samples, n_input = X.shape\n",
    "    W1 = np.random.randn(n_input, n_hidden)\n",
    "    b1 = np.zeros((1, n_hidden))\n",
    "    W2 = np.random.randn(n_hidden,)  # Reshape W2 to (n_hidden,)\n",
    "    b2 = np.zeros((1,))\n",
    "    \n",
    "    Y_reshaped = y.reshape(-1, 1)  # Reshape y for broadcasting\n",
    "\n",
    "    for i in range(1, n_iter+1):\n",
    "        Z2 = np.matmul(X, W1) + b1\n",
    "        A2 = sigmoid(Z2)\n",
    "        Z3 = np.matmul(A2, W2) + b2\n",
    "        A3 = Z3\n",
    "        \n",
    "        Y_reshaped = Y_reshaped.reshape(Y_reshaped.shape[0], 1, 1)\n",
    "        Y_broadcasted = np.broadcast_to(Y_reshaped, (Y_reshaped.shape[0], n_samples, 1))\n",
    "        dZ3 = A3 - Y_broadcasted\n",
    "\n",
    "        db2 = np.sum(dZ3, axis=0, keepdims=True)\n",
    "        \n",
    "        dZ2 = np.matmul(dZ3, W2) * sigmoid_derivative(Z2)\n",
    "        dW1 = np.matmul(X.transpose((0, 2, 1)), dZ2)\n",
    "        db1 = np.sum(dZ2, axis=0)\n",
    "        \n",
    "        W2 = W2 - learning_rate * np.mean(dW1, axis=(0, 2), keepdims=True) / m\n",
    "        b2 = b2 - learning_rate * db2 / m\n",
    "        W1 = W1 - learning_rate * dW1 / m\n",
    "        b1 = b1 - learning_rate * db1 / m\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            cost = np.mean((y.reshape(-1, 1) - A3) ** 2)\n",
    "            print('Iteration %i, training loss: %f' % (i, cost))\n",
    "    \n",
    "    model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60-ok8N5lEdM"
   },
   "source": [
    "شبکه ساده"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "M_de6A6SgPNH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "# Split data into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Optionally, you can scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_train = y_train.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WKCAmyKfhEuZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Initialize weights and biases for the hidden layer\n",
    "        self.W1 = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.b1 = np.zeros((1, self.hidden_size))\n",
    "\n",
    "        # Initialize weights and biases for the output layer\n",
    "        self.W2 = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.b2 = np.zeros((1, self.output_size))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Compute the activation of the hidden layer\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "\n",
    "        # Compute the activation of the output layer\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "\n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        m = X.shape[0]\n",
    "\n",
    "        # Compute the gradients of the output layer\n",
    "        dZ2 = self.a2 - y\n",
    "        dW2 = np.dot(self.a1.T, dZ2) / m\n",
    "        db2 = np.sum(dZ2, axis=0) / m\n",
    "\n",
    "        # Compute the gradients of the hidden layer\n",
    "        dZ1 = np.dot(dZ2, self.W2.T) * self.sigmoid_derivative(self.z1)\n",
    "        dW1 = np.dot(X.T, dZ1) / m\n",
    "        db1 = np.sum(dZ1, axis=0) / m\n",
    "        \n",
    "\n",
    "        # Update the weights and biases\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "\n",
    "    def train(self, X, y, num_iterations, learning_rate):\n",
    "        for i in range(num_iterations):\n",
    "            # Forward pass\n",
    "            output = self.forward(X)\n",
    "\n",
    "            # Backward pass\n",
    "            self.backward(X, y, learning_rate)\n",
    "\n",
    "            # Compute the loss (mean squared error)\n",
    "            loss = np.mean((output - y) ** 2)\n",
    "\n",
    "            # Print the loss every 100 iterations\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Iteration: {i+1}, Loss: {loss}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lnfcwTTQhOhP",
    "outputId": "79aa41ca-d3f0-49e4-9bf5-2aaaa0cb00f8"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration: 100, Loss: 181.70327896039603\n",
      "Iteration: 200, Loss: 181.70327896039603\n",
      "Iteration: 300, Loss: 181.70327896039603\n",
      "Iteration: 400, Loss: 181.70327896039603\n",
      "Iteration: 500, Loss: 181.70327896039603\n",
      "Iteration: 600, Loss: 181.70327896039603\n",
      "Iteration: 700, Loss: 181.70327896039603\n",
      "Iteration: 800, Loss: 181.70327896039603\n",
      "Iteration: 900, Loss: 181.70327896039603\n",
      "Iteration: 1000, Loss: 181.70327896039603\n",
      "Iteration: 1100, Loss: 181.70327896039603\n",
      "Iteration: 1200, Loss: 181.70327896039603\n",
      "Iteration: 1300, Loss: 181.70327896039603\n",
      "Iteration: 1400, Loss: 181.70327896039603\n",
      "Iteration: 1500, Loss: 181.70327896039603\n",
      "Iteration: 1600, Loss: 181.70327896039603\n",
      "Iteration: 1700, Loss: 181.70327896039603\n",
      "Iteration: 1800, Loss: 181.70327896039603\n",
      "Iteration: 1900, Loss: 181.70327896039603\n",
      "Iteration: 2000, Loss: 181.70327896039603\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_size = X_train_scaled.shape[1]\n",
    "hidden_size = 20\n",
    "output_size = 1\n",
    "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "\n",
    "num_iterations = 2000\n",
    "learning_rate = 0.01\n",
    "nn.train(X_train_scaled, y_train, num_iterations, learning_rate)\n",
    "\n",
    "\n",
    "predictions = nn.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "q9D6tRgBOVrQ"
   },
   "outputs": [],
   "source": [
    "def predict(x, model):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    A2 = sigmoid(np.matmul(x, W1) + b1)\n",
    "    A3 = np.matmul(A2, W2) + b2\n",
    "    return A3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35q0ny_BOIPi",
    "outputId": "34c5dc81-5f1b-4708-b6e3-476454162184"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "[ 9.04  3.53 18.07  5.52 17.27 11.97 18.33 24.16 12.87 14.33 17.92 17.1\n",
      " 36.98 12.34 11.74 11.66 17.58 30.62  2.97 18.13  6.59  7.19 22.6   9.67\n",
      " 15.7  18.14  9.1  18.71 17.27 14.1  14.69  7.2  13.44 10.19 21.32 15.79\n",
      "  4.03  9.8  14.66  7.54 21.14  7.53  2.88 13.09  8.81 17.11 18.35  6.72\n",
      " 16.29  4.98 13.27  4.59 18.72  7.67  3.16 10.58 16.59  3.33  9.62 18.46\n",
      "  9.5   4.81  3.76 11.69  5.49 23.09 17.12  6.72  4.32 19.01 14.15  7.74\n",
      " 23.79  5.57 10.27 30.59 12.33  3.81 21.08 23.97  8.79 26.4   9.25 25.68\n",
      " 11.72  8.05 16.42  6.12  6.21 14.59 10.11 26.64 12.26 12.67  7.12 11.12\n",
      "  9.53 34.37 18.05 21.22 12.86  8.94]\n"
     ]
    }
   ],
   "source": [
    "predictions =nn.predict(X_test_scaled)\n",
    "print(predictions)\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ut0eOlb8lPGV",
    "outputId": "0edf4205-d554-49ac-be0f-e8db5fc7d675"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration: 100, Loss: 15.211037728114418\n",
      "Iteration: 200, Loss: 14.062381624329538\n",
      "Iteration: 300, Loss: 13.363452068615628\n",
      "Iteration: 400, Loss: 12.879076781652618\n",
      "Iteration: 500, Loss: 12.462048419537995\n",
      "Iteration: 600, Loss: 12.093595215671106\n",
      "Iteration: 700, Loss: 11.759182541603238\n",
      "Iteration: 800, Loss: 11.437996288224205\n",
      "Iteration: 900, Loss: 11.137668595457017\n",
      "Iteration: 1000, Loss: 10.8217759907913\n",
      "Iteration: 1100, Loss: 10.484417048679699\n",
      "Iteration: 1200, Loss: 10.187900224396142\n",
      "Iteration: 1300, Loss: 9.912695388573876\n",
      "Iteration: 1400, Loss: 9.647090473410262\n",
      "Iteration: 1500, Loss: 9.385938640481887\n",
      "Iteration: 1600, Loss: 9.157585683836515\n",
      "Iteration: 1700, Loss: 8.954543598879262\n",
      "Iteration: 1800, Loss: 8.772357708568581\n",
      "Iteration: 1900, Loss: 8.610979161719218\n",
      "Iteration: 2000, Loss: 8.45509568952375\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "\n",
    "\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "\n",
    "        self.W1 = np.random.randn(self.input_size, self.hidden_size) * np.sqrt(2 / self.input_size)\n",
    "        self.b1 = np.zeros((1, self.hidden_size))\n",
    "\n",
    "        self.W2 = np.random.randn(self.hidden_size, self.output_size) * np.sqrt(2 / self.hidden_size)\n",
    "        self.b2 = np.zeros((1, self.output_size))\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "\n",
    "\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.z2\n",
    "\n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        m = X.shape[0]\n",
    "\n",
    "        dZ2 = self.a2 - y\n",
    "        dW2 = np.dot(self.a1.T, dZ2) / m\n",
    "        db2 = np.sum(dZ2, axis=0) / m\n",
    "\n",
    "\n",
    "        dZ1 = np.dot(dZ2, self.W2.T) * self.relu_derivative(self.z1)\n",
    "        dW1 = np.dot(X.T, dZ1) / m\n",
    "        db1 = np.sum(dZ1, axis=0) / m\n",
    "\n",
    "\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "\n",
    "    def train(self, X, y, num_iterations, learning_rate, print_loss=True):\n",
    "        for i in range(num_iterations):\n",
    "            # Forward pass\n",
    "            output = self.forward(X)\n",
    "\n",
    "            # Backward pass\n",
    "            self.backward(X, y, learning_rate)\n",
    "\n",
    "            # Compute the loss (mean squared error)\n",
    "            loss = np.mean((output - y) ** 2)\n",
    "\n",
    "            # Print the loss every 100 iterations\n",
    "            if print_loss and (i + 1) % 100 == 0:\n",
    "                print(f\"Iteration: {i+1}, Loss: {loss}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def relu_derivative(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "\n",
    "input_size = X_train_scaled.shape[1]\n",
    "hidden_size = 20\n",
    "output_size = 1\n",
    "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "num_iterations = 2000\n",
    "learning_rate = 0.01\n",
    "nn.train(X_train_scaled, y_train, num_iterations, learning_rate)\n",
    "\n",
    "\n",
    "predictions = nn.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2BYRmf2CmIM2"
   },
   "outputs": [],
   "source": [
    "def predict(x, model):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    A2 = sigmoid(np.matmul(x, W1) + b1)\n",
    "    A3 = np.matmul(A2, W2) + b2\n",
    "    return A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NMNS4ccHmO8e",
    "outputId": "3aac8074-c15b-4b17-b7ba-3a29b9cb0ed4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 9.99771142]\n",
      " [ 5.63973921]\n",
      " [16.84433669]\n",
      " [11.71203502]\n",
      " [15.50672045]\n",
      " [10.06495186]\n",
      " [14.30685415]\n",
      " [15.67089815]\n",
      " [11.52816692]\n",
      " [11.71129966]\n",
      " [17.41718516]\n",
      " [18.91612963]\n",
      " [22.23399525]\n",
      " [ 9.01298428]\n",
      " [13.77876572]\n",
      " [11.95962322]\n",
      " [19.83989688]\n",
      " [26.58978447]\n",
      " [ 6.26964545]\n",
      " [18.48605776]\n",
      " [ 8.99750728]\n",
      " [ 7.71945222]\n",
      " [18.43276347]\n",
      " [ 6.48828881]\n",
      " [18.48458117]\n",
      " [15.08838506]\n",
      " [13.01788542]\n",
      " [23.76247401]\n",
      " [18.46454602]\n",
      " [12.40915076]\n",
      " [16.35949781]\n",
      " [ 6.34077463]\n",
      " [18.88258389]\n",
      " [19.76454092]\n",
      " [13.06383519]\n",
      " [18.13976655]\n",
      " [ 3.38875715]\n",
      " [10.3015607 ]\n",
      " [13.98311244]\n",
      " [ 9.91023056]\n",
      " [17.46255457]\n",
      " [ 8.51931774]\n",
      " [ 5.27284141]\n",
      " [11.6650258 ]\n",
      " [ 7.82410983]\n",
      " [19.38586338]\n",
      " [18.69237129]\n",
      " [ 8.68044642]\n",
      " [12.8379146 ]\n",
      " [ 8.2014658 ]\n",
      " [17.94355898]\n",
      " [ 5.53415397]\n",
      " [12.94930446]\n",
      " [ 6.09845312]\n",
      " [ 4.00953753]\n",
      " [10.88344506]\n",
      " [17.15147754]\n",
      " [ 4.70538831]\n",
      " [ 7.06700938]\n",
      " [15.21554954]\n",
      " [ 4.2400568 ]\n",
      " [ 2.84867228]\n",
      " [ 6.92823375]\n",
      " [13.74595659]\n",
      " [ 7.19314161]\n",
      " [24.09619986]\n",
      " [18.2986468 ]\n",
      " [ 6.76478012]\n",
      " [ 5.52576321]\n",
      " [20.11797771]\n",
      " [11.55883562]\n",
      " [11.09507235]\n",
      " [20.84902929]\n",
      " [ 8.01382033]\n",
      " [10.62158052]\n",
      " [27.63027892]\n",
      " [13.63556827]\n",
      " [ 5.6365602 ]\n",
      " [21.27459981]\n",
      " [21.14873988]\n",
      " [ 9.45244175]\n",
      " [15.69358554]\n",
      " [ 8.14467553]\n",
      " [25.26327393]\n",
      " [13.7303617 ]\n",
      " [ 8.48778752]\n",
      " [15.86685319]\n",
      " [ 5.79868263]\n",
      " [10.8419289 ]\n",
      " [12.94696101]\n",
      " [ 9.03072289]\n",
      " [17.61972121]\n",
      " [13.8119876 ]\n",
      " [13.52694054]\n",
      " [28.13782263]\n",
      " [13.25097098]\n",
      " [20.10161801]\n",
      " [16.04552377]\n",
      " [25.44979648]\n",
      " [23.16269134]\n",
      " [ 9.71171868]\n",
      " [ 8.99552628]]\n",
      "[ 9.04  3.53 18.07  5.52 17.27 11.97 18.33 24.16 12.87 14.33 17.92 17.1\n",
      " 36.98 12.34 11.74 11.66 17.58 30.62  2.97 18.13  6.59  7.19 22.6   9.67\n",
      " 15.7  18.14  9.1  18.71 17.27 14.1  14.69  7.2  13.44 10.19 21.32 15.79\n",
      "  4.03  9.8  14.66  7.54 21.14  7.53  2.88 13.09  8.81 17.11 18.35  6.72\n",
      " 16.29  4.98 13.27  4.59 18.72  7.67  3.16 10.58 16.59  3.33  9.62 18.46\n",
      "  9.5   4.81  3.76 11.69  5.49 23.09 17.12  6.72  4.32 19.01 14.15  7.74\n",
      " 23.79  5.57 10.27 30.59 12.33  3.81 21.08 23.97  8.79 26.4   9.25 25.68\n",
      " 11.72  8.05 16.42  6.12  6.21 14.59 10.11 26.64 12.26 12.67  7.12 11.12\n",
      "  9.53 34.37 18.05 21.22 12.86  8.94]\n"
     ]
    }
   ],
   "source": [
    "predictions =nn.predict(X_test_scaled)\n",
    "print(predictions)\n",
    "print(y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
